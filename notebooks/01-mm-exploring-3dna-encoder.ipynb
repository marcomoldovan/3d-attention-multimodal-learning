{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuwa_pytorch.nuwa_pytorch import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(dim=128, depth=4, sparse_3dna_attn=True, sparse_3dna_video_shape=[8, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[8, 16, 16][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = transformer(torch.randn(32, 64, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuwa_pytorch.nuwa_pytorch import Sparse3DNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_3dna = Sparse3DNA(dim=128, video_shape=[10, 32, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames, fmap_size, _ = sparse_3dna.video_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_3dna.heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = torch.randn(1, 10, 3, 256, 256) # (batch, frames, channels, height, width)\n",
    "imgs = torch.randn(10, 3, 256, 256) # (batch, channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sparse_3dna(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10241, 128])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 512\n",
    "max_video_frames = 5\n",
    "image_size = 256\n",
    "vae_num_layers = 4\n",
    "fmap_size = image_size // (2 ** vae_num_layers)\n",
    "video_shape = (max_video_frames, fmap_size, fmap_size)\n",
    "sparse_3dna_dilation = 1\n",
    "sparse_3dna_dilations = tuple(range(1, sparse_3dna_dilation + 1)) if not isinstance(sparse_3dna_dilation, (list, tuple)) else sparse_3dna_dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuwa_pytorch.vqgan_vae import VQGanVAE\n",
    "\n",
    "from nuwa_pytorch.nuwa_pytorch import Embedding, Transformer\n",
    "\n",
    "vae = VQGanVAE(\n",
    "    dim = 64,\n",
    "    num_layers = 4,\n",
    "    image_size = 256,\n",
    "    num_conv_blocks = 2,\n",
    "    vq_codebook_size = 8192\n",
    ") \n",
    "\n",
    "num_image_tokens = vae.codebook_size\n",
    "\n",
    "image_embedding = Embedding(num_image_tokens, dim, frac_gradient=0.2)\n",
    "\n",
    "encoder = Transformer(\n",
    "            dim = dim,\n",
    "            depth = 6,\n",
    "            heads = 8,\n",
    "            dim_head = 64,\n",
    "            causal = True,\n",
    "            cross_attend = True,\n",
    "            attn_dropout = 0.,\n",
    "            ff_dropout = 0.,\n",
    "            ff_chunk_size = None,\n",
    "            shift_video_tokens = True,\n",
    "            sparse_3dna_video_shape = video_shape,\n",
    "            sparse_3dna_attn = True,\n",
    "            sparse_3dna_kernel_size = 3,\n",
    "            sparse_3dna_dilations = sparse_3dna_dilations,\n",
    "            sparse_3dna_query_num_frames_chunk = None,\n",
    "            sparse_3dna_rel_pos_bias = False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat\n",
    "\n",
    "from nuwa_pytorch.nuwa_pytorch import AxialPositionalEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1279) must match the size of tensor b (2560) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\marco\\Google Drive\\Projects\\3D Attention for Self-Supervised Multimodal Learning\\notebooks\\01-mm-exploring-3dna-encoder.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marco/Google%20Drive/Projects/3D%20Attention%20for%20Self-Supervised%20Multimodal%20Learning/notebooks/01-mm-exploring-3dna-encoder.ipynb#ch0000018?line=21'>22</a>\u001b[0m frame_indices_input \u001b[39m=\u001b[39m frame_indices[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m return_loss \u001b[39melse\u001b[39;00m frame_indices\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marco/Google%20Drive/Projects/3D%20Attention%20for%20Self-Supervised%20Multimodal%20Learning/notebooks/01-mm-exploring-3dna-encoder.ipynb#ch0000018?line=22'>23</a>\u001b[0m frame_embeddings \u001b[39m=\u001b[39m image_embedding(frame_indices_input)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/marco/Google%20Drive/Projects/3D%20Attention%20for%20Self-Supervised%20Multimodal%20Learning/notebooks/01-mm-exploring-3dna-encoder.ipynb#ch0000018?line=23'>24</a>\u001b[0m frame_embeddings \u001b[39m=\u001b[39m video_pos_emb()[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39m+\u001b[39;49m frame_embeddings\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marco/Google%20Drive/Projects/3D%20Attention%20for%20Self-Supervised%20Multimodal%20Learning/notebooks/01-mm-exploring-3dna-encoder.ipynb#ch0000018?line=25'>26</a>\u001b[0m bos \u001b[39m=\u001b[39m repeat(video_bos, \u001b[39m'\u001b[39m\u001b[39md -> b 1 d\u001b[39m\u001b[39m'\u001b[39m, b \u001b[39m=\u001b[39m batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marco/Google%20Drive/Projects/3D%20Attention%20for%20Self-Supervised%20Multimodal%20Learning/notebooks/01-mm-exploring-3dna-encoder.ipynb#ch0000018?line=26'>27</a>\u001b[0m frame_embeddings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((bos, frame_embeddings), dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (1279) must match the size of tensor b (2560) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "dim = 512\n",
    "max_video_frames = 5\n",
    "image_size = vae.image_size\n",
    "fmap_size = image_size // (2 ** vae.num_layers)\n",
    "\n",
    "# preparing text for the encoder\n",
    "text = torch.randint(0, 20000, (1, 256))\n",
    "batch, seq_len, frames, device = *text.shape, video.shape[1], text.device\n",
    "\n",
    "# preparing image for the encoder\n",
    "imgs = torch.randn(10, 3, 256, 256) # (batch, channels, height, width)\n",
    "\n",
    "# preparing video for the encoder\n",
    "video = torch.randn(1, 10, 3, 256, 256) # (batch, frames, channels, height, width)\n",
    "video_shape = (max_video_frames, fmap_size, fmap_size)\n",
    "video_pos_emb = AxialPositionalEmbedding(dim, shape = video_shape)\n",
    "video_bos = torch.nn.Parameter(torch.randn(dim))\n",
    "\n",
    "return_loss = False\n",
    "frame_indices = vae.get_video_indices(video)\n",
    "frame_indices = rearrange(frame_indices, 'b ... -> b (...)')\n",
    "frame_indices_input = frame_indices[:, :-1] if return_loss else frame_indices\n",
    "frame_embeddings = image_embedding(frame_indices_input)\n",
    "frame_embeddings = video_pos_emb()[:-1] + frame_embeddings\n",
    "\n",
    "bos = repeat(video_bos, 'd -> b 1 d', b = batch)\n",
    "frame_embeddings = torch.cat((bos, frame_embeddings), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1279, 512])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_pos_emb()[:-1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_outputs = encoder(frame_embeddings)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ee050b4bb971ac4df86c0279d33f37be55b07a3fa21ee13455fb882afd6196d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('3d-attention-multimodal-learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
